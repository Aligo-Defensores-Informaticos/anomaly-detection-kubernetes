{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874a67dc-838b-4a73-80df-9485bfe00fb8",
      "metadata": {
        "id": "874a67dc-838b-4a73-80df-9485bfe00fb8"
      },
      "outputs": [],
      "source": [
        "import kfp\n",
        "import kfp.dsl as dsl\n",
        "from kfp.components import create_component_from_func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6136ff15-d347-4960-9966-317da9243643",
      "metadata": {
        "id": "6136ff15-d347-4960-9966-317da9243643"
      },
      "outputs": [],
      "source": [
        "def get_data() -> list:\n",
        "    import requests\n",
        "    \n",
        "    errors = float('inf')\n",
        "    while errors > 0:\n",
        "        errors = 0\n",
        "\n",
        "        BASE_URL = 'http://0.0.0.0:0000/api/v1/query'\n",
        "\n",
        "        content = {\n",
        "            'query': \"sum(kube_pod_status_scheduled{condition=\\\"false\\\"})\",\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            'Content-Type': 'application/x-www-form-urlencoded'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            scheduling_failed_pods = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(apiserver_request_total[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            api_server_request_rate = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'avg(apiserver_request_duration_seconds_sum / apiserver_request_duration_seconds_count)'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            api_server_request_latency = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(workqueue_depth)'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            workqueue_depth = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(histogram_quantile(0.99, sum(scheduler_e2e_scheduling_duration_seconds_bucket) by (le, instance)))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            scheduler_e2e_scheduling_latency = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(scheduler_preemption_attempts_total[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            scheduler_preemption_attempts_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(nginx_ingress_controller_nginx_process_connections{state=\\'active\\'})'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            active_client_connections = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(nginx_ingress_controller_nginx_process_connections{state=\\'reading\\'})'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            reading_client_connections = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(nginx_ingress_controller_nginx_process_connections{state=\\'waiting\\'})'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            waiting_client_connections = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(nginx_ingress_controller_nginx_process_connections{state=\\'writing\\'})'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            writing_client_connections = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "\n",
        "        namespace = 'cattle-neuvector-system'\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_cpu_cfs_throttled_seconds_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_cpu_cfs_throttled_seconds_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            container_cpu_cfs_throttled_seconds_total = 0\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_cpu_usage_seconds_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_cpu_usage_seconds_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_cpu_system_seconds_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_cpu_system_seconds_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_cpu_user_seconds_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_cpu_user_seconds_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(container_memory_working_set_bytes{namespace=\\'' + namespace + '\\'})'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_memory_working_set_bytes = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_network_receive_packets_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_receive_packets_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_network_receive_packets_dropped_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_receive_packets_dropped_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_network_receive_errors_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_receive_errors_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_network_transmit_packets_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_transmit_packets_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_network_transmit_packets_dropped_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_transmit_packets_dropped_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_network_transmit_errors_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_transmit_errors_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_network_receive_bytes_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_receive_bytes_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_network_transmit_bytes_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_transmit_bytes_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_fs_reads_bytes_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_fs_reads_bytes_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content['query'] = 'sum(rate(container_fs_writes_bytes_total{namespace=\\'' + namespace + '\\'}[5m]))'\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_fs_writes_bytes_total = response['data']['result'][0]['value'][1]\n",
        "        except:\n",
        "            errors += 1\n",
        "            \n",
        "\n",
        "    flow = [0] * 25\n",
        "    flow[0] = float(api_server_request_latency)\n",
        "    flow[1] = float(api_server_request_rate)\n",
        "    flow[2] = float(scheduling_failed_pods)\n",
        "    flow[3] = float(workqueue_depth)\n",
        "    flow[4] = float(scheduler_e2e_scheduling_latency)\n",
        "    flow[5] = float(scheduler_preemption_attempts_total)\n",
        "    flow[6] = float(active_client_connections)\n",
        "    flow[7] = float(reading_client_connections)\n",
        "    flow[8] = float(waiting_client_connections)\n",
        "    flow[9] = float(writing_client_connections)\n",
        "    flow[10] = float(container_cpu_cfs_throttled_seconds_total)\n",
        "    flow[11] = float(container_cpu_usage_seconds_total)\n",
        "    flow[12] = float(container_cpu_system_seconds_total)\n",
        "    flow[13] = float(container_cpu_user_seconds_total)\n",
        "    flow[14] = float(container_memory_working_set_bytes)\n",
        "    flow[15] = float(container_network_receive_packets_total)\n",
        "    flow[16] = float(container_network_receive_packets_dropped_total) \n",
        "    flow[17] = float(container_network_receive_errors_total)\n",
        "    flow[18] = float(container_network_transmit_packets_total)\n",
        "    flow[19] = float(container_network_transmit_packets_dropped_total)\n",
        "    flow[20] = float(container_network_transmit_errors_total)\n",
        "    flow[21] = float(container_network_receive_bytes_total)\n",
        "    flow[22] = float(container_network_transmit_bytes_total)\n",
        "    flow[23] = float(container_fs_reads_bytes_total)\n",
        "    flow[24] = float(container_fs_writes_bytes_total)\n",
        "    \n",
        "    return flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c3bea84-ce89-4ffe-ae70-eeb31b6938dc",
      "metadata": {
        "id": "6c3bea84-ce89-4ffe-ae70-eeb31b6938dc"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(flow: list) -> dict:\n",
        "    import boto3\n",
        "    import joblib\n",
        "    import tempfile\n",
        "    \n",
        "    s3 = boto3.resource('s3',\n",
        "        aws_access_key_id='----',\n",
        "        aws_secret_access_key='----',\n",
        "        region_name='----',\n",
        "    )\n",
        "    \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        s3.Bucket('----').download_fileobj(Key=\"kubeflow/current_run_preprocessing.sav\", Fileobj=file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        current_run_preprocessing = joblib.load(file)\n",
        "        \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        key = current_run_preprocessing[\"min_max_scaler_path\"]\n",
        "        \n",
        "        s3.Bucket('----').download_fileobj(Key=key, Fileobj=file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        min_max_conf = joblib.load(file)\n",
        "    \n",
        "    flow_mm = [0] * 25\n",
        "    for i in range(25):\n",
        "        minimum = min_max_conf[i][0]\n",
        "        maximum = min_max_conf[i][1]\n",
        "\n",
        "        if maximum - minimum == 0:\n",
        "            flow_mm[i] = 0.0\n",
        "        else:\n",
        "            X_std = (flow[i] - minimum) / (maximum - minimum)\n",
        "            X_scaled = X_std * (1 - 0) + 0\n",
        "            flow_mm[i] = X_scaled\n",
        "            \n",
        "\n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        key = current_run_preprocessing[\"std_scaler_path\"]\n",
        "        \n",
        "        s3.Bucket('----').download_fileobj(Key=key, Fileobj=file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        std_scaler_conf = joblib.load(file)\n",
        "    \n",
        "    flow_ss = [0] * 25\n",
        "    for i in range(25):\n",
        "        mean = std_scaler_conf[i][0]\n",
        "        std_dev = std_scaler_conf[i][1]\n",
        "\n",
        "        if std_dev == 0:\n",
        "            flow_ss[i] = 0.0\n",
        "        else:\n",
        "            X_scaled = (flow[i] - mean) / std_dev\n",
        "            flow_ss[i] = X_scaled\n",
        "            \n",
        "    \n",
        "    return {\n",
        "        \"flow_mm\": flow_mm,\n",
        "        \"flow_ss\": flow_ss,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19d35532-1166-4c93-94c5-8ddd7fd620b3",
      "metadata": {
        "id": "19d35532-1166-4c93-94c5-8ddd7fd620b3"
      },
      "outputs": [],
      "source": [
        "def classify(transformed_data: dict) -> float:\n",
        "    import boto3\n",
        "    import tempfile\n",
        "    import joblib\n",
        "    \n",
        "    s3 = boto3.resource('s3',\n",
        "        aws_access_key_id='----',\n",
        "        aws_secret_access_key='----',\n",
        "        region_name='----',\n",
        "    )\n",
        "    \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        s3.Bucket('----').download_fileobj(Key=\"kubeflow/current_run_ml_models.sav\", Fileobj=file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        current_run_ml_models = joblib.load(file)\n",
        "    \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        key = current_run_ml_models[\"osvm_path\"]\n",
        "        \n",
        "        s3.Bucket('----').download_fileobj(Key=key, Fileobj=file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        osvm = joblib.load(file)\n",
        "        \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        key = current_run_ml_models[\"isolation_forest_path\"]\n",
        "        \n",
        "        s3.Bucket('----').download_fileobj(Key=key, Fileobj=file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        isolation_forest = joblib.load(file)\n",
        "        \n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    class Autoencoder(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Autoencoder, self).__init__()\n",
        "\n",
        "            # Encoder\n",
        "            self.enc_layer1 = nn.Linear(25, 12)\n",
        "            self.enc_layer2 = nn.Linear(12, 9)\n",
        "            self.enc_layer3 = nn.Linear(9, 6)\n",
        "\n",
        "            # Decoder\n",
        "            self.dec_layer1 = nn.Linear(6, 9)\n",
        "            self.dec_layer2 = nn.Linear(9, 12)\n",
        "            self.dec_layer3 = nn.Linear(12, 25)\n",
        "\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.enc_layer1(x))\n",
        "            x = F.relu(self.enc_layer2(x))\n",
        "            x = F.relu(self.enc_layer3(x))\n",
        "            x = F.relu(self.dec_layer1(x))\n",
        "            x = F.relu(self.dec_layer2(x))\n",
        "            x = F.relu(self.dec_layer3(x))\n",
        "\n",
        "            return x\n",
        "\n",
        "    ae = Autoencoder()\n",
        "    \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        s3.Bucket('----').download_fileobj(Key=\"kubeflow/current_run_dl_model.sav\", Fileobj=file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        current_run_dl_model = joblib.load(file)\n",
        "        \n",
        "    \n",
        "    import io\n",
        "    from io import BytesIO\n",
        "    \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        key = current_run_dl_model[\"ae_path\"]\n",
        "        \n",
        "        s3.Bucket('----').download_fileobj(Key=key, Fileobj=file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        buffer = BytesIO(file.read())\n",
        "        \n",
        "        ae.load_state_dict(torch.load(buffer))\n",
        "        ae.eval()\n",
        "    \n",
        "    \n",
        "    import numpy as np\n",
        "    \n",
        "    flow_mm = transformed_data[\"flow_mm\"]\n",
        "    flow_mm = np.array(flow_mm).reshape(1, -1)\n",
        "    res_osvm = osvm.predict(flow_mm)\n",
        "\n",
        "    if res_osvm[0] == -1:\n",
        "        # Outlier\n",
        "        score_osvm = 0.1\n",
        "    else:\n",
        "        # Inlier\n",
        "        score_osvm = 0.0\n",
        "    \n",
        "    import sklearn\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    flow_mm_torch = torch.from_numpy(flow_mm).type(torch.FloatTensor)\n",
        "    flow_mm_torch = torch.autograd.Variable(flow_mm_torch)\n",
        "    prediction = ae(flow_mm_torch)\n",
        "    prediction = prediction.detach().numpy()\n",
        "    mse = mean_squared_error(y_true=flow_mm, y_pred=prediction)\n",
        "    \n",
        "    ae_threshold = current_run_dl_model[\"ae_threshold\"]\n",
        "    res_ae = mse > ae_threshold\n",
        "\n",
        "    if res_ae:\n",
        "        # Outlier\n",
        "        score_ae = 0.6\n",
        "    else:\n",
        "        # Inlier\n",
        "        score_ae = 0.0\n",
        "    \n",
        "    flow_ss = transformed_data[\"flow_ss\"]\n",
        "    flow_ss = np.array(flow_ss).reshape(1, -1)\n",
        "    res_isolation_forest = isolation_forest.predict(flow_ss)\n",
        "\n",
        "    if res_isolation_forest[0] == -1:\n",
        "        # Outlier\n",
        "        score_isolation_forest = 0.3\n",
        "    else:\n",
        "        # Inlier\n",
        "        score_isolation_forest = 0.0\n",
        "    \n",
        "    outlier_confidence_score = score_osvm + score_ae + score_isolation_forest\n",
        "    \n",
        "    if outlier_confidence_score > 0.9:\n",
        "        outlier_confidence_score = 1.0\n",
        "    elif outlier_confidence_score > 0.8:\n",
        "        outlier_confidence_score = 0.9\n",
        "    \n",
        "    return outlier_confidence_score * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b919d34a-b528-416f-9024-1db9421cf699",
      "metadata": {
        "id": "b919d34a-b528-416f-9024-1db9421cf699"
      },
      "outputs": [],
      "source": [
        "def report_classification(outlier_confidence_score: float):\n",
        "    import boto3\n",
        "    import tempfile\n",
        "    import joblib\n",
        "    from datetime import datetime\n",
        "    \n",
        "    s3 = boto3.resource('s3',\n",
        "        aws_access_key_id='----',\n",
        "        aws_secret_access_key='----',\n",
        "        region_name='----',\n",
        "    )\n",
        "    \n",
        "    report_result = True\n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        try:\n",
        "            s3.Bucket('----').download_fileobj(Key=\"kubeflow/last_time_anomaly_detected.sav\", Fileobj=file)\n",
        "\n",
        "            file.seek(0)\n",
        "\n",
        "            last_time = joblib.load(file)\n",
        "\n",
        "            diff = datetime.now() - last_time\n",
        "\n",
        "            total_minutes = diff.total_seconds() / 60\n",
        "\n",
        "            if total_minutes < 15:\n",
        "                report_result = False\n",
        "            else:\n",
        "                with tempfile.TemporaryFile() as new_file:\n",
        "                    joblib.dump(datetime.now(), new_file)\n",
        "\n",
        "                    new_file.seek(0)\n",
        "\n",
        "                    key = \"kubeflow/last_time_anomaly_detected.sav\"\n",
        "\n",
        "                    s3.Bucket('----').put_object(Key=key, Body=new_file.read())\n",
        "        except:\n",
        "            joblib.dump(datetime.now(), file)\n",
        "\n",
        "            file.seek(0)\n",
        "\n",
        "            key = \"kubeflow/last_time_anomaly_detected.sav\"\n",
        "\n",
        "            s3.Bucket('----').put_object(Key=key, Body=file.read())\n",
        "    \n",
        "    \n",
        "    if report_result:\n",
        "        import requests\n",
        "\n",
        "        bot_token = \"----\"\n",
        "        bot_chat_id = \"----\"\n",
        "\n",
        "        message = \"*Anomaly detected*\\n- Outlier confidence score: {}%\".format(outlier_confidence_score)\n",
        "\n",
        "        send_text = f\"https://api.telegram.org/bot{bot_token}/sendMessage?parse_mode=Markdown&chat_id={bot_chat_id}&text={message}\"\n",
        "\n",
        "        try:\n",
        "            requests.get(send_text)\n",
        "        except:\n",
        "            print(\"Error when trying to send the Telegram message\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ebac6d-1beb-4d52-8810-e699f8d4920f",
      "metadata": {
        "id": "a0ebac6d-1beb-4d52-8810-e699f8d4920f"
      },
      "outputs": [],
      "source": [
        "get_data_op = create_component_from_func(\n",
        "    func=get_data,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\",\n",
        ")\n",
        "\n",
        "preprocess_data_op = create_component_from_func(\n",
        "    func=preprocess_data,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\",\n",
        "    packages_to_install=[\"boto3\"],\n",
        ")\n",
        "\n",
        "classify_op = create_component_from_func(\n",
        "    func=classify,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\",\n",
        "    packages_to_install=[\"boto3\"],\n",
        ")\n",
        "\n",
        "report_anomaly_op = create_component_from_func(\n",
        "    func=report_classification,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\",\n",
        "    packages_to_install=[\"boto3\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb935a11-1287-4a45-b7fe-25b60eea635d",
      "metadata": {
        "id": "bb935a11-1287-4a45-b7fe-25b60eea635d"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    name=\"Testing pipeline for Aligo Smart Detective\",\n",
        "    description=\"AI Monitor for Aligo\\s DevSecOps Infrastructure\",\n",
        ")\n",
        "def testing_pipeline():\n",
        "    # First step\n",
        "    data = get_data_op()\n",
        "    \n",
        "    # Second step\n",
        "    transformed_data = preprocess_data_op(data.output)\n",
        "    \n",
        "    # Third step\n",
        "    classification = classify_op(transformed_data.output)\n",
        "    \n",
        "    with dsl.Condition(classification.output >= 90.0):\n",
        "        # Fourth step\n",
        "        report_anomaly_op(classification.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90de8eb0-7142-4aa9-b9e7-ce22fd4efb6f",
      "metadata": {
        "id": "90de8eb0-7142-4aa9-b9e7-ce22fd4efb6f"
      },
      "outputs": [],
      "source": [
        "kfp.compiler.Compiler().compile(testing_pipeline, \"testing_pipeline.zip\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}