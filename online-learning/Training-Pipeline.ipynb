{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874a67dc-838b-4a73-80df-9485bfe00fb8",
      "metadata": {
        "id": "874a67dc-838b-4a73-80df-9485bfe00fb8"
      },
      "outputs": [],
      "source": [
        "import kfp\n",
        "import kfp.dsl as dsl\n",
        "from kfp.components import InputPath, OutputPath, create_component_from_func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6717d9-b5c9-4b43-b8c6-93b47611f982",
      "metadata": {
        "id": "ee6717d9-b5c9-4b43-b8c6-93b47611f982"
      },
      "outputs": [],
      "source": [
        "def create_dataset(output_csv: OutputPath('csv')):\n",
        "    from datetime import date, timedelta\n",
        "    import requests\n",
        "    import time\n",
        "\n",
        "    today = date.today()\n",
        "    end = f\"{today}T17:00:00.00Z\"\n",
        "    \n",
        "    days = 6\n",
        "    last_day = date.today() - timedelta(days=days)\n",
        "    start = f\"{last_day}T17:00:00.00Z\"\n",
        "    \n",
        "    total_records = 60 * 24 * days + 1\n",
        "    \n",
        "    errors = float('inf')\n",
        "    while errors > 0:\n",
        "        errors = 0\n",
        "        \n",
        "        # Sleep for 1 minute to avoid any HTTP requests limit error\n",
        "        time.sleep(60)\n",
        "\n",
        "        BASE_URL = \"http://0.0.0.0:0000/api/v1/query_range\"\n",
        "        step = \"1m\"\n",
        "\n",
        "        content = {\n",
        "            \"query\": \"sum(kube_pod_status_scheduled{condition=\\\"false\\\"})\",\n",
        "            \"start\": start,\n",
        "            \"end\": end,\n",
        "            \"step\": step,\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            scheduling_failed_pods = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(apiserver_request_total[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            api_server_request_rate = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"avg(apiserver_request_duration_seconds_sum / apiserver_request_duration_seconds_count)\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            api_server_request_latency = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(workqueue_depth)\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            workqueue_depth = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(histogram_quantile(0.99, sum(scheduler_e2e_scheduling_duration_seconds_bucket) by (le, instance)))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            scheduler_e2e_scheduling_latency = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(scheduler_preemption_attempts_total[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            scheduler_preemption_attempts_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(nginx_ingress_controller_nginx_process_connections{state=\\\"active\\\"})\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            active_client_connections = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(nginx_ingress_controller_nginx_process_connections{state=\\\"reading\\\"})\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            reading_client_connections = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(nginx_ingress_controller_nginx_process_connections{state=\\\"waiting\\\"})\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            waiting_client_connections = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(nginx_ingress_controller_nginx_process_connections{state=\\\"writing\\\"})\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            writing_client_connections = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "\n",
        "        namespace = \"cattle-neuvector-system\"\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_cpu_cfs_throttled_seconds_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_cpu_cfs_throttled_seconds_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            container_cpu_cfs_throttled_seconds_total = [[0, 0]] * total_records\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_cpu_usage_seconds_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_cpu_usage_seconds_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_cpu_system_seconds_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_cpu_system_seconds_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_cpu_user_seconds_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_cpu_user_seconds_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(container_memory_working_set_bytes{namespace=\\\"\" + namespace + \"\\\"})\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_memory_working_set_bytes = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_network_receive_packets_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_receive_packets_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_network_receive_packets_dropped_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_receive_packets_dropped_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_network_receive_errors_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_receive_errors_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_network_transmit_packets_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_transmit_packets_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_network_transmit_packets_dropped_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_transmit_packets_dropped_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_network_transmit_errors_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_transmit_errors_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_network_receive_bytes_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_receive_bytes_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_network_transmit_bytes_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_network_transmit_bytes_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_fs_reads_bytes_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_fs_reads_bytes_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        try:\n",
        "            content[\"query\"] = \"sum(rate(container_fs_writes_bytes_total{namespace=\\\"\" + namespace + \"\\\"}[5m]))\"\n",
        "            response = requests.post(BASE_URL, content, headers).json()\n",
        "            container_fs_writes_bytes_total = response[\"data\"][\"result\"][0][\"values\"]\n",
        "        except:\n",
        "            errors += 1\n",
        "        \n",
        "    \n",
        "    import pandas as pd\n",
        "\n",
        "    timestamps = pd.period_range(start=f\"{last_day} 12:00\", end=f\"{today} 12:00\", freq=\"T\")\n",
        "    data = []\n",
        "    for i in range(total_records):\n",
        "        data.append({\n",
        "            \"timestamp\": str(timestamps[i]),\n",
        "            \"api_server_request_latency\": float(api_server_request_latency[i][1]),\n",
        "            \"api_server_request_rate\": float(api_server_request_rate[i][1]),\n",
        "            \"scheduling_failed_pods\": float(scheduling_failed_pods[i][1]),\n",
        "            \"workqueue_depth\": float(workqueue_depth[i][1]),\n",
        "            \"scheduler_e2e_scheduling_latency\": float(scheduler_e2e_scheduling_latency[i][1]),\n",
        "            \"scheduler_preemption_attempts_total\": float(scheduler_preemption_attempts_total[i][1]),\n",
        "            \"active_client_connections\": float(active_client_connections[i][1]),\n",
        "            \"reading_client_connections\": float(reading_client_connections[i][1]),\n",
        "            \"waiting_client_connections\": float(waiting_client_connections[i][1]),\n",
        "            \"writing_client_connections\": float(writing_client_connections[i][1]),\n",
        "            \"container_cpu_cfs_throttled_seconds_total\": float(container_cpu_cfs_throttled_seconds_total[i][1]),\n",
        "            \"container_cpu_usage_seconds_total\": float(container_cpu_usage_seconds_total[i][1]),\n",
        "            \"container_cpu_system_seconds_total\": float(container_cpu_system_seconds_total[i][1]),\n",
        "            \"container_cpu_user_seconds_total\": float(container_cpu_user_seconds_total[i][1]),\n",
        "            \"container_memory_working_set_bytes\": float(container_memory_working_set_bytes[i][1]),\n",
        "            \"container_network_receive_packets_total\": float(container_network_receive_packets_total[i][1]),\n",
        "            \"container_network_receive_packets_dropped_total\": float(container_network_receive_packets_dropped_total[i][1]), \n",
        "            \"container_network_receive_errors_total\": float(container_network_receive_errors_total[i][1]),\n",
        "            \"container_network_transmit_packets_total\": float(container_network_transmit_packets_total[i][1]),\n",
        "            \"container_network_transmit_packets_dropped_total\": float(container_network_transmit_packets_dropped_total[i][1]),\n",
        "            \"container_network_transmit_errors_total\": float(container_network_transmit_errors_total[i][1]),\n",
        "            \"container_network_receive_bytes_total\": float(container_network_receive_bytes_total[i][1]),\n",
        "            \"container_network_transmit_bytes_total\": float(container_network_transmit_bytes_total[i][1]),\n",
        "            \"container_fs_reads_bytes_total\": float(container_fs_reads_bytes_total[i][1]),\n",
        "            \"container_fs_writes_bytes_total\": float(container_fs_writes_bytes_total[i][1]),\n",
        "        })\n",
        "\n",
        "    dataset = pd.DataFrame(data)\n",
        "    \n",
        "    dataset.to_csv(output_csv, index=False)\n",
        "    \n",
        "    import boto3\n",
        "    import uuid\n",
        "    from io import StringIO\n",
        "    \n",
        "    csv_buffer = StringIO()\n",
        "    dataset.to_csv(csv_buffer, index=False)\n",
        "    \n",
        "    s3 = boto3.resource('s3',\n",
        "        aws_access_key_id='----',\n",
        "        aws_secret_access_key='----',\n",
        "        region_name='----',\n",
        "    )\n",
        "    \n",
        "    identifier = str(uuid.uuid4())\n",
        "    key = f\"kubeflow/datasets/{identifier}.csv\"\n",
        "    \n",
        "    s3.Bucket('----').put_object(Key=key, Body=csv_buffer.getvalue())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd9a196-78b2-4e5b-9e46-d04d2c5e18a8",
      "metadata": {
        "id": "edd9a196-78b2-4e5b-9e46-d04d2c5e18a8"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(input_dataset: InputPath('csv'), output_transformed_data: OutputPath('sav')):\n",
        "    import numpy as np\n",
        "    import sklearn\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "    \n",
        "    import pandas as pd\n",
        "    \n",
        "    dataset = pd.read_csv(input_dataset)\n",
        "    \n",
        "    total_records = 60 * 24 * 6 + 1\n",
        "    \n",
        "    X = dataset.drop([\"timestamp\"], axis=1)\n",
        "    y = np.array([1] * total_records)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    X_train_mm = min_max_scaler.fit_transform(X_train)\n",
        "    X_test_mm = min_max_scaler.transform(X_test)\n",
        "\n",
        "    std_scaler = StandardScaler()\n",
        "    X_train_ss = std_scaler.fit_transform(X_train)\n",
        "    X_test_ss = std_scaler.transform(X_test)\n",
        "    \n",
        "    import joblib\n",
        "    import tempfile\n",
        "    import boto3\n",
        "    import uuid\n",
        "    \n",
        "    s3 = boto3.resource('s3',\n",
        "        aws_access_key_id='----',\n",
        "        aws_secret_access_key='----',\n",
        "        region_name='----',\n",
        "    )\n",
        "    \n",
        "    min_max_conf = X_train.apply(lambda x: pd.Series([x.min(), x.max()])).T.values.tolist()\n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        joblib.dump(min_max_conf, file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        identifier = str(uuid.uuid4())\n",
        "        min_max_key = f\"kubeflow/preprocessing/min_max_scaler/{identifier}.sav\"\n",
        "    \n",
        "        s3.Bucket('----').put_object(Key=min_max_key, Body=file.read())\n",
        "    \n",
        "    std_scaler_conf = X_train.apply(lambda x: pd.Series([x.mean(), x.std()])).T.values.tolist()\n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        joblib.dump(std_scaler_conf, file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        identifier = str(uuid.uuid4())\n",
        "        std_scaler_key = f\"kubeflow/preprocessing/standard_scaler/{identifier}.sav\"\n",
        "        \n",
        "        s3.Bucket('----').put_object(Key=std_scaler_key, Body=file.read()) \n",
        "    \n",
        "    current_run_preprocessing = {\n",
        "        \"min_max_scaler_path\": min_max_key,\n",
        "        \"std_scaler_path\": std_scaler_key,\n",
        "    }\n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        joblib.dump(current_run_preprocessing, file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        key = \"kubeflow/current_run_preprocessing.sav\"\n",
        "        \n",
        "        s3.Bucket('----').put_object(Key=key, Body=file.read()) \n",
        "\n",
        "    output_data = {\n",
        "        \"X\": X,\n",
        "        \"y\": y,\n",
        "        \"X_train\": X_train,\n",
        "        \"y_train\": y_train,\n",
        "        \"X_test\": X_test,\n",
        "        \"y_test\": y_test,\n",
        "        \"X_train_mm\": X_train_mm,\n",
        "        \"X_test_mm\": X_test_mm,\n",
        "        \"X_train_ss\": X_train_ss,\n",
        "        \"X_test_ss\": X_test_ss,\n",
        "    }\n",
        "    joblib.dump(output_data, output_transformed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53d973f-7d9e-4ed7-b250-10e3dfb1d824",
      "metadata": {
        "id": "f53d973f-7d9e-4ed7-b250-10e3dfb1d824"
      },
      "outputs": [],
      "source": [
        "def train_ml_models(input_dataset: InputPath('sav')) -> dict:\n",
        "    \n",
        "    import sklearn\n",
        "    from sklearn.svm import OneClassSVM\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    \n",
        "    import joblib\n",
        "    \n",
        "    transformed_data = joblib.load(input_dataset)\n",
        "    \n",
        "    osvm = OneClassSVM(kernel=\"rbf\", nu=0.05, gamma=\"auto\").fit(transformed_data[\"X_train_mm\"])\n",
        "    y_pred = osvm.predict(transformed_data[\"X_test_mm\"])\n",
        "    osvm_acc = accuracy_score(y_true=transformed_data[\"y_test\"], y_pred=y_pred)\n",
        "    \n",
        "    isolation_forest = IsolationForest(n_estimators=50, max_samples=256, contamination=0.01).fit(transformed_data[\"X_train_ss\"])\n",
        "    y_pred = isolation_forest.predict(transformed_data[\"X_test_ss\"])\n",
        "    isolation_forest_acc = accuracy_score(y_true=transformed_data[\"y_test\"], y_pred=y_pred)\n",
        "    \n",
        "    import boto3\n",
        "    import tempfile\n",
        "    import uuid\n",
        "    \n",
        "    s3 = boto3.resource('s3',\n",
        "        aws_access_key_id='----',\n",
        "        aws_secret_access_key='----',\n",
        "        region_name='----',\n",
        "    )\n",
        "    \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        joblib.dump(osvm, file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        identifier = str(uuid.uuid4())\n",
        "        osvm_key = f\"kubeflow/ai_models/osvm/{identifier}.sav\"\n",
        "    \n",
        "        s3.Bucket('----').put_object(Key=osvm_key, Body=file.read())\n",
        "        \n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        joblib.dump(isolation_forest, file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        identifier = str(uuid.uuid4())\n",
        "        isolation_forest_key = f\"kubeflow/ai_models/isolation_forest/{identifier}.sav\"\n",
        "        \n",
        "        s3.Bucket('----').put_object(Key=isolation_forest_key, Body=file.read())\n",
        "        \n",
        "    \n",
        "    current_run_ml_models = {\n",
        "        \"osvm_path\": osvm_key,\n",
        "        \"isolation_forest_path\": isolation_forest_key,\n",
        "    }\n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        joblib.dump(current_run_ml_models, file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        key = \"kubeflow/current_run_ml_models.sav\"\n",
        "        \n",
        "        s3.Bucket('----').put_object(Key=key, Body=file.read())\n",
        "    \n",
        "    \n",
        "    return {\n",
        "        \"osvm_acc\": osvm_acc,\n",
        "        \"isolation_forest_acc\": isolation_forest_acc,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cc9a277-7c01-403a-b62f-941fc8d2d3e8",
      "metadata": {
        "id": "2cc9a277-7c01-403a-b62f-941fc8d2d3e8"
      },
      "outputs": [],
      "source": [
        "def train_dl_model(input_dataset: InputPath('sav')) -> dict:\n",
        "    import numpy as np\n",
        "    \n",
        "    import sklearn\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    \n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torch.autograd import Variable\n",
        "    from torch.utils.data import TensorDataset\n",
        "    \n",
        "    import joblib\n",
        "    \n",
        "    transformed_data = joblib.load(input_dataset)\n",
        "    \n",
        "    input_dim = transformed_data[\"X_train\"].shape[1]\n",
        "    num_epochs = 10\n",
        "    batch_size = 256\n",
        "    \n",
        "    class Autoencoder(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Autoencoder, self).__init__()\n",
        "            \n",
        "            # Encoder\n",
        "            self.enc_layer1 = nn.Linear(input_dim, 12)\n",
        "            self.enc_layer2 = nn.Linear(12, 9)\n",
        "            self.enc_layer3 = nn.Linear(9, 6)\n",
        "\n",
        "            # Decoder\n",
        "            self.dec_layer1 = nn.Linear(6, 9)\n",
        "            self.dec_layer2 = nn.Linear(9, 12)\n",
        "            self.dec_layer3 = nn.Linear(12, input_dim)\n",
        "\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.enc_layer1(x))\n",
        "            x = F.relu(self.enc_layer2(x))\n",
        "            x = F.relu(self.enc_layer3(x))\n",
        "            x = F.relu(self.dec_layer1(x))\n",
        "            x = F.relu(self.dec_layer2(x))\n",
        "            x = F.relu(self.dec_layer3(x))\n",
        "\n",
        "            return x\n",
        "        \n",
        "    X_train_mm_torch = torch.from_numpy(transformed_data[\"X_train_mm\"]).type(torch.FloatTensor)\n",
        "    y_train_torch = torch.from_numpy(transformed_data[\"y_train\"])\n",
        "\n",
        "    X_test_mm_torch = torch.from_numpy(transformed_data[\"X_test_mm\"]).type(torch.FloatTensor)\n",
        "    y_test_torch = torch.from_numpy(transformed_data[\"y_test\"])\n",
        "\n",
        "    train_mm = TensorDataset(X_train_mm_torch, y_train_torch)\n",
        "    test_mm = TensorDataset(X_test_mm_torch, y_test_torch)\n",
        "\n",
        "    train_mm_dataloader = torch.utils.data.DataLoader(train_mm, batch_size=batch_size, shuffle=True, num_workers=3)\n",
        "    test_mm_dataloader = torch.utils.data.DataLoader(test_mm, batch_size=batch_size, shuffle=True, num_workers=3)\n",
        "    \n",
        "    ae = Autoencoder()\n",
        "    \n",
        "    loss_func = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (data, target) in enumerate(train_mm_dataloader):\n",
        "            data = torch.autograd.Variable(data)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = ae(data)\n",
        "            loss = loss_func(pred, data)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        current_loss = loss.item()\n",
        "    \n",
        "    ae.eval()\n",
        "    predictions = []\n",
        "    for batch_idx, (data,target) in enumerate(train_mm_dataloader):\n",
        "        data = torch.autograd.Variable(data)\n",
        "        pred = ae(data)\n",
        "        for prediction in pred:\n",
        "            predictions.append(prediction.detach().numpy())\n",
        "    \n",
        "    mse = np.mean(np.power(transformed_data[\"X_train_mm\"] - predictions, 2), axis=1)\n",
        "    ae_threshold = np.mean(mse) + np.std(mse)\n",
        "    \n",
        "    predictions = []\n",
        "    for batch_idx, (data,target) in enumerate(test_mm_dataloader):\n",
        "        data = torch.autograd.Variable(data)\n",
        "        pred = ae(data)\n",
        "        for prediction in pred:\n",
        "            predictions.append(prediction.detach().numpy())\n",
        "            \n",
        "    mse = np.mean(np.power(transformed_data[\"X_test_mm\"] - predictions, 2), axis=1)\n",
        "    avg_mse = np.mean(mse)\n",
        "    \n",
        "    import boto3\n",
        "    import uuid\n",
        "    import tempfile\n",
        "    from io import BytesIO\n",
        "    \n",
        "    s3 = boto3.resource('s3',\n",
        "        aws_access_key_id='----',\n",
        "        aws_secret_access_key='----',\n",
        "        region_name='----',\n",
        "    )\n",
        "    \n",
        "    ae_buffer = BytesIO()\n",
        "    torch.save(ae.state_dict(), ae_buffer)\n",
        "    \n",
        "    identifier = str(uuid.uuid4())\n",
        "    ae_key = f\"kubeflow/ai_models/autoencoder/{identifier}.pt\"\n",
        "    \n",
        "    s3.Bucket('----').put_object(Key=ae_key, Body=ae_buffer.getvalue())\n",
        "    \n",
        "    \n",
        "    current_run_dl_model = {\n",
        "        \"ae_path\": ae_key,\n",
        "        \"ae_threshold\": ae_threshold,\n",
        "    }\n",
        "    with tempfile.TemporaryFile() as file:\n",
        "        joblib.dump(current_run_dl_model, file)\n",
        "        \n",
        "        file.seek(0)\n",
        "        \n",
        "        key = \"kubeflow/current_run_dl_model.sav\"\n",
        "        \n",
        "        s3.Bucket('----').put_object(Key=key, Body=file.read())\n",
        "    \n",
        "    \n",
        "    return {\n",
        "        \"current_loss\": current_loss,\n",
        "        \"avg_mse\": avg_mse,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b919d34a-b528-416f-9024-1db9421cf699",
      "metadata": {
        "id": "b919d34a-b528-416f-9024-1db9421cf699"
      },
      "outputs": [],
      "source": [
        "def report_training_results(ml_output: dict, dl_output: dict):\n",
        "    import requests\n",
        "    \n",
        "    bot_token = \"----\"\n",
        "    bot_chat_id = \"----\"\n",
        "    \n",
        "    message = \"*Training pipeline finished*\\n*Results*\\n- OSVM accuracy: {:.3f}\\n- Isolation Forest accuracy: {:.3f}\\n- Autoencoder loss: {:.4f}\".format(ml_output[\"osvm_acc\"], ml_output[\"isolation_forest_acc\"], dl_output[\"current_loss\"])\n",
        "\n",
        "    send_text = f\"https://api.telegram.org/bot{bot_token}/sendMessage?parse_mode=Markdown&chat_id={bot_chat_id}&text={message}\"\n",
        "    \n",
        "    try:\n",
        "        requests.get(send_text)\n",
        "    except:\n",
        "        print(\"Error when trying to send the Telegram message\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ebac6d-1beb-4d52-8810-e699f8d4920f",
      "metadata": {
        "id": "a0ebac6d-1beb-4d52-8810-e699f8d4920f"
      },
      "outputs": [],
      "source": [
        "create_dataset_op = create_component_from_func(\n",
        "    func=create_dataset,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\",\n",
        "    packages_to_install=[\"boto3\"],\n",
        ")\n",
        "\n",
        "preprocess_dataset_op = create_component_from_func(\n",
        "    func=preprocess_dataset,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\",\n",
        "    packages_to_install=[\"boto3\"],\n",
        ")\n",
        "\n",
        "train_ml_models_op = create_component_from_func(\n",
        "    func=train_ml_models,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\",\n",
        "    packages_to_install=[\"boto3\"],\n",
        ")\n",
        "\n",
        "train_dl_model_op = create_component_from_func(\n",
        "    func=train_dl_model,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\",\n",
        "    packages_to_install=[\"boto3\"],\n",
        ")\n",
        "\n",
        "report_training_results_op = create_component_from_func(\n",
        "    func=report_training_results,\n",
        "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu@sha256:b6f7894a4118a5bf51ebf7ae83a444b32e34828c40d6a780d6ff2d3c34818ffd\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb935a11-1287-4a45-b7fe-25b60eea635d",
      "metadata": {
        "id": "bb935a11-1287-4a45-b7fe-25b60eea635d"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    name=\"Training pipeline for Aligo Smart Detective\",\n",
        "    description=\"AI Monitor for Aligo\\s DevSecOps Infrastructure\",\n",
        ")\n",
        "def training_pipeline():\n",
        "    # First step\n",
        "    dataset = create_dataset_op()\n",
        "    \n",
        "    # Second step\n",
        "    transformed_dataset = preprocess_dataset_op(dataset.outputs[\"output_csv\"])\n",
        "    \n",
        "    # Third step (in parallel)\n",
        "    ml_res = train_ml_models_op(transformed_dataset.outputs[\"output_transformed_data\"])\n",
        "    dl_res = train_dl_model_op(transformed_dataset.outputs[\"output_transformed_data\"])\n",
        "    \n",
        "    # Fourth step\n",
        "    report_training_results_op(ml_res.output, dl_res.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90de8eb0-7142-4aa9-b9e7-ce22fd4efb6f",
      "metadata": {
        "id": "90de8eb0-7142-4aa9-b9e7-ce22fd4efb6f"
      },
      "outputs": [],
      "source": [
        "kfp.compiler.Compiler().compile(training_pipeline, \"training_pipeline.zip\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}